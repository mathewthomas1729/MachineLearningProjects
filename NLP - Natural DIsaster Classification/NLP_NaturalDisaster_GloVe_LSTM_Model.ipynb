{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import tensorflow\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM,Dropout,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading the data\n",
    "df = pd.read_csv('data/train.csv',encoding='utf-8')\n",
    "holdout = pd.read_csv('data/test.csv',encoding='utf-8')\n",
    "df.head(7)\n",
    "holdout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    " 1. Remove URL \n",
    " 2. Remove emojis \n",
    " 3. Remove Contractions \n",
    " 4. Remove Punctuations\n",
    " 5. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    urls = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text_without_https =  urls.sub(r'',text)\n",
    "    url = re.compile(r'http?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text_without_https)\n",
    "\n",
    "\n",
    "#remove emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "#remove contractions\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "\"thx\"   : \"thanks\"\n",
    "}\n",
    "\n",
    "def remove_contractions(text):\n",
    "    temp = text.split()\n",
    "    final_text =''\n",
    "    for wrd in temp:\n",
    "        if wrd.lower() in contractions:\n",
    "            replaced_word = contractions[wrd.lower()] + ' '\n",
    "        else:\n",
    "            replaced_word = wrd + ' '\n",
    "        final_text  = final_text + replaced_word\n",
    "    return final_text.strip()\n",
    "\n",
    "\n",
    "\n",
    "#remove punctuation\n",
    "import string\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "df['text']=df['text'].apply(remove_URL)\n",
    "df['text']=df['text'].apply(remove_emoji)\n",
    "df['text']=df['text'].apply(remove_contractions)\n",
    "df['text']=df['text'].apply(remove_punct)\n",
    "\n",
    "\n",
    "holdout['text']=holdout['text'].apply(remove_URL)\n",
    "holdout['text']=holdout['text'].apply(remove_emoji)\n",
    "holdout['text']=holdout['text'].apply(remove_contractions)\n",
    "holdout['text']=holdout['text'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:03<00:00, 2389.93it/s]\n",
      "100%|██████████| 3263/3263 [00:01<00:00, 2236.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['deeds', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'us'], ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada'], ['residents', 'asked', 'shelter', 'place', 'notified', 'officers', 'evacuation', 'shelter', 'place', 'orders', 'expected']]\n",
      "---------\n",
      "[['happened', 'terrible', 'car', 'crash'], ['heard', 'earthquake', 'different', 'cities', 'stay', 'safe', 'everyone'], ['forest', 'fire', 'spot', 'pond', 'geese', 'fleeing', 'across', 'street', 'save']]\n",
      "(3263, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_tweets = list()\n",
    "holdout_tweets = list()\n",
    "lines =df['text'].values.tolist()\n",
    "for line in tqdm(lines):\n",
    "  token = word_tokenize(line)\n",
    "  token = [word.lower() for word in token]\n",
    "  table = str.maketrans('','',string.punctuation)\n",
    "  stripped = [word.translate(table) for word in token]\n",
    "  words = [word for word in stripped if word.isalpha()]\n",
    "  stopped_words = set(stopwords.words('english'))\n",
    "  final_token = [w for w in words if w not in stopped_words]\n",
    "  train_tweets.append(final_token)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lines =holdout['text'].values.tolist()\n",
    "for line in tqdm(lines):\n",
    "  token = word_tokenize(line)\n",
    "  token = [word.lower() for word in token]\n",
    "  table = str.maketrans('','',string.punctuation)\n",
    "  stripped = [word.translate(table) for word in token]\n",
    "  words = [word for word in stripped if word.isalpha()]\n",
    "  stopped_words = set(stopwords.words('english'))\n",
    "  final_token = [w for w in words if w not in stopped_words]\n",
    "  holdout_tweets.append(final_token)\n",
    "    \n",
    "    \n",
    "print(train_tweets[:3])\n",
    "print(\"---------\")\n",
    "print(holdout_tweets[:3])\n",
    "print(holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 636131.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Number of words')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking distrubtion of length of tweets\n",
    "%matplotlib inline\n",
    "\n",
    "len_list=[]\n",
    "for i in tqdm(tweets):\n",
    "    len_list.append(len(i))\n",
    "\n",
    "plt.hist(len_list, density=False, bins=10)  \n",
    "plt.ylabel('Count of tweets')\n",
    "plt.xlabel('Number of words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tweets have less than 20 words.\n",
    "Next step is to create a dictionary of words with n dimensions which will be used in the model. Will be using GloVe embedding with 300 dimensions which can be downloaded from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_glove = {}\n",
    "f = codecs.open('embeddings/glove.840B.300d.txt', encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index_glove[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43674  , -0.10333  ,  0.33635  ,  0.051283 , -0.19348  ,\n",
       "       -0.20316  , -0.4466   ,  1.0433   ,  0.26677  ,  1.7239   ,\n",
       "       -0.28369  , -0.26205  , -0.45438  , -0.30005  , -0.61656  ,\n",
       "        0.62768  , -0.23625  ,  0.91878  ,  0.065177 , -0.63918  ,\n",
       "        0.12668  ,  0.17995  , -0.19695  ,  0.19099  , -0.10736  ,\n",
       "       -0.50168  , -0.55079  , -0.19555  ,  0.1151   , -0.017492 ,\n",
       "        0.091201 , -0.78987  , -0.43777  ,  0.15285  ,  0.053872 ,\n",
       "        0.020404 , -0.18169  ,  0.072808 , -0.28677  ,  0.13125  ,\n",
       "       -0.11851  ,  0.077099 ,  0.36826  , -0.30647  ,  0.061576 ,\n",
       "       -0.39104  , -0.082649 , -0.37517  ,  0.19079  , -0.020282 ,\n",
       "       -0.30688  ,  0.12898  ,  0.20518  ,  0.42767  , -1.0231   ,\n",
       "        0.068287 ,  0.15214  , -0.12757  , -0.020682 ,  0.045102 ,\n",
       "       -0.32932  ,  0.39246  , -0.062813 ,  0.20911  , -0.20522  ,\n",
       "        0.079021 , -0.12221  ,  0.61051  , -0.25953  ,  0.39264  ,\n",
       "        0.2195   ,  0.31411  , -0.308    ,  0.38777  ,  0.45427  ,\n",
       "        0.18622  ,  0.52402  ,  0.16743  , -0.23737  , -0.26981  ,\n",
       "       -0.14362  ,  0.28071  ,  0.18986  ,  0.33935  ,  0.2429   ,\n",
       "       -0.17622  ,  0.23582  ,  0.28532  , -0.098001 , -0.078731 ,\n",
       "        0.041204 , -0.041776 , -0.90437  , -0.3762   ,  0.0054902,\n",
       "       -0.42204  , -0.073251 ,  0.01995  ,  0.30391  , -0.086289 ,\n",
       "       -0.16177  ,  0.096598 , -0.66345  ,  0.22192  , -0.68919  ,\n",
       "       -1.1884   , -0.58696  , -0.0054178, -0.52044  , -1.292    ,\n",
       "       -0.10527  ,  0.4255   ,  0.092851 , -0.039084 ,  0.23891  ,\n",
       "        0.25218  , -0.27589  , -0.077557 , -0.56887  , -0.207    ,\n",
       "       -0.54322  ,  0.64327  , -0.062978 , -0.397    , -0.16045  ,\n",
       "       -0.10282  , -0.2106   , -0.46952  , -0.20994  ,  0.3152   ,\n",
       "       -0.34023  , -0.2133   ,  0.096743 ,  0.15442  ,  0.67719  ,\n",
       "        0.86989  , -0.39999  , -0.0020942, -0.10015  ,  0.43654  ,\n",
       "       -1.3582   ,  0.35732  ,  0.03313  ,  0.36099  ,  0.41803  ,\n",
       "        0.29635  , -0.030912 ,  0.50614  , -0.185    ,  0.20418  ,\n",
       "       -0.14335  , -0.17479  , -0.11267  ,  0.19287  ,  0.10123  ,\n",
       "       -0.6003   ,  0.35167  , -0.099055 , -0.16425  ,  0.55548  ,\n",
       "        0.99715  ,  0.63761  , -0.71995  ,  0.12506  , -0.14968  ,\n",
       "        0.46844  ,  0.25031  ,  0.49816  ,  0.52479  , -0.13662  ,\n",
       "        0.20231  , -0.0080016,  0.0018195,  0.61953  , -0.26155  ,\n",
       "       -0.47353  ,  0.9387   , -0.54263  , -0.13323  ,  0.20378  ,\n",
       "        0.0040551,  0.15076  , -0.32122  ,  0.27246  ,  0.11412  ,\n",
       "       -0.29578  ,  0.30562  ,  0.18893  , -0.29123  , -0.15471  ,\n",
       "        0.44317  , -0.20312  , -0.054718 , -0.010005 , -0.47384  ,\n",
       "        0.65387  ,  0.098169 ,  0.43473  ,  0.073944 ,  0.58485  ,\n",
       "       -0.36164  , -0.021684 ,  0.99232  ,  0.021888 ,  0.22645  ,\n",
       "       -0.69601  ,  0.15724  , -0.20026  ,  0.23598  ,  0.69433  ,\n",
       "       -0.4075   , -0.097319 ,  0.42575  ,  0.53023  , -0.52438  ,\n",
       "        0.061352 , -0.14567  , -0.60826  , -0.21166  , -0.70275  ,\n",
       "        0.064854 ,  0.0053817,  0.67858  , -0.40032  ,  0.069151 ,\n",
       "       -0.36074  , -0.29544  , -0.73902  , -0.50519  , -0.11898  ,\n",
       "       -0.33916  ,  0.029804 , -0.30147  , -0.060449 ,  0.034127 ,\n",
       "       -0.13152  ,  0.29968  , -0.45115  ,  0.12281  , -0.2631   ,\n",
       "        0.41465  , -0.78259  ,  0.38437  ,  0.18649  , -0.46933  ,\n",
       "        0.2888   ,  1.0869   ,  0.28095  ,  0.56347  , -0.1848   ,\n",
       "       -0.14738  ,  0.56361  , -0.16911  ,  0.1604   , -0.13425  ,\n",
       "       -0.064711 , -0.42178  , -0.56966  ,  0.17     ,  0.46265  ,\n",
       "        0.21811  ,  0.37346  ,  0.08215  , -0.70342  ,  0.3447   ,\n",
       "       -0.45242  , -0.27433  ,  0.56628  , -0.94808  ,  0.072123 ,\n",
       "       -0.14476  ,  0.2002   , -0.29541  , -0.48663  ,  0.60329  ,\n",
       "       -0.1313   ,  0.24571  , -0.28702  , -0.24956  , -0.17805  ,\n",
       "       -0.29612  , -0.63691  ,  0.26325  ,  0.15592  ,  0.41142  ,\n",
       "       -0.079347 ,  0.90255  , -0.51769  ,  0.33698  ,  0.56252  ,\n",
       "        0.048539 ,  0.55867  ,  0.18634  , -0.034428 ,  0.11681  ,\n",
       "        0.20489  , -0.38604  ,  0.32031  , -0.5498   , -0.19245  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spot checking the embedding index dictionary\n",
    "embeddings_index_glove['hurricane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1218, 20)\n",
      "(4873, 20)\n",
      "(1522, 20)\n",
      "(3263, 20)\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN_OF_TWEET = 20\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(train_tweets)\n",
    "train_sequences=tokenizer_obj.texts_to_sequences(train_tweets)\n",
    "train_tweet_pad = pad_sequences(train_sequences,maxlen=MAX_LEN_OF_TWEET)\n",
    "word_index = tokenizer_obj.word_index\n",
    "is_disaster = np.array(df['target'])\n",
    "\n",
    "\n",
    "num_validation_sample=int(VALIDATION_SPLIT*train_tweet_pad.shape[0])\n",
    "x_val_pad = train_tweet_pad[:num_validation_sample]\n",
    "y_val = is_disaster[:num_validation_sample]\n",
    "x_train_pad = train_tweet_pad[num_validation_sample:] \n",
    "y_train = is_disaster[num_validation_sample:]\n",
    "\n",
    "num_test_sample=int(TEST_SPLIT*x_train_pad.shape[0])\n",
    "x_test_pad = train_tweet_pad[:num_test_sample]\n",
    "y_test = is_disaster[:num_test_sample]\n",
    "x_train_pad = x_train_pad[num_test_sample:] \n",
    "y_train = y_train[num_test_sample:]\n",
    "\n",
    "\n",
    "\n",
    "holdout_sequences=tokenizer_obj.texts_to_sequences(holdout_tweets)\n",
    "holdout_tweet_pad = pad_sequences(holdout_sequences,maxlen=MAX_LEN_OF_TWEET)\n",
    "x_holdout = holdout_tweet_pad\n",
    "\n",
    "print(x_test_pad.shape)\n",
    "print(x_train_pad.shape)\n",
    "print(x_val_pad.shape)\n",
    "print(x_holdout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Embedding Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words =len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "  if i>num_words:\n",
    "    continue\n",
    "  embedding_vector = embeddings_index_glove.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of built model ..\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 300)           4828200   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20, 300)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20, 32)            9632      \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 20, 128)           49664     \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 20, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 20, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 20, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,227,273\n",
      "Trainable params: 399,073\n",
      "Non-trainable params: 4,828,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,EMBEDDING_DIM,input_length=MAX_LEN_OF_TWEET, weights=[embedding_matrix],trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print('Summary of built model ..')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the model.fit() method to a variable, which will store the Training, Validation Loss and Accuracy for each epoch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "39/39 - 25s - loss: 0.2421 - accuracy: 0.9015 - val_loss: 0.5803 - val_accuracy: 0.8049\n",
      "Epoch 2/15\n",
      "39/39 - 29s - loss: 0.2340 - accuracy: 0.9070 - val_loss: 0.5635 - val_accuracy: 0.7898\n",
      "Epoch 3/15\n",
      "39/39 - 28s - loss: 0.2352 - accuracy: 0.9007 - val_loss: 0.5824 - val_accuracy: 0.8009\n",
      "Epoch 4/15\n",
      "39/39 - 22s - loss: 0.2310 - accuracy: 0.9070 - val_loss: 0.5837 - val_accuracy: 0.7884\n",
      "Epoch 5/15\n",
      "39/39 - 28s - loss: 0.2432 - accuracy: 0.9019 - val_loss: 0.6106 - val_accuracy: 0.7852\n",
      "Epoch 6/15\n",
      "39/39 - 25s - loss: 0.2338 - accuracy: 0.9079 - val_loss: 0.6143 - val_accuracy: 0.7858\n",
      "Epoch 7/15\n",
      "39/39 - 24s - loss: 0.2152 - accuracy: 0.9124 - val_loss: 0.5987 - val_accuracy: 0.7825\n",
      "Epoch 8/15\n",
      "39/39 - 20s - loss: 0.2657 - accuracy: 0.8988 - val_loss: 0.5701 - val_accuracy: 0.7898\n",
      "Epoch 9/15\n",
      "39/39 - 18s - loss: 0.2246 - accuracy: 0.9120 - val_loss: 0.6254 - val_accuracy: 0.7930\n",
      "Epoch 10/15\n",
      "39/39 - 18s - loss: 0.2258 - accuracy: 0.9136 - val_loss: 0.6009 - val_accuracy: 0.8003\n",
      "Epoch 11/15\n",
      "39/39 - 18s - loss: 0.2119 - accuracy: 0.9169 - val_loss: 0.5814 - val_accuracy: 0.7852\n",
      "Epoch 12/15\n",
      "39/39 - 22s - loss: 0.2381 - accuracy: 0.9036 - val_loss: 0.6173 - val_accuracy: 0.7989\n",
      "Epoch 13/15\n",
      "39/39 - 23s - loss: 0.2182 - accuracy: 0.9070 - val_loss: 0.6514 - val_accuracy: 0.8003\n",
      "Epoch 14/15\n",
      "39/39 - 21s - loss: 0.1988 - accuracy: 0.9185 - val_loss: 0.6372 - val_accuracy: 0.7917\n",
      "Epoch 15/15\n",
      "39/39 - 18s - loss: 0.2082 - accuracy: 0.9196 - val_loss: 0.5969 - val_accuracy: 0.7924\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_pad,y_train,batch_size=128,epochs=20,validation_data=(x_val_pad,y_val),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model on Test Set\n",
    "\n",
    "**Score** is the evaluation of the loss function for a given input.\n",
    "\n",
    "**Accuracy** How accurate your model's prediction is compared to the true data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 0.5603 - accuracy: 0.8095\n",
      "Test Loss score  0.5603271126747131\n",
      "Test Accuracy score  0.8095238208770752\n"
     ]
    }
   ],
   "source": [
    "loss_score,accuracy_score = model.evaluate(x_test_pad,y_test,batch_size=128)\n",
    "print('Test Loss score ',loss_score)\n",
    "print('Test Accuracy score ',accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model on Sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7129875 ],\n",
       "       [0.13017455],\n",
       "       [0.9987523 ]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample_1= 'hurricane in chennai'\n",
    "test_sample_2= 'beware world ablaze sierra leone'\n",
    "test_sample_3 = 'wild fire happening in bangalore '\n",
    "\n",
    "test_samples = [test_sample_1,test_sample_2,test_sample_3]\n",
    "test_sample_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
    "test_sample_tokens_pad = pad_sequences(test_sample_tokens,maxlen=20)\n",
    "\n",
    "#predict\n",
    "model.predict(x=test_sample_tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 20)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(x_holdout)>0.5).astype(int).shape\n",
    "sample_submission.shape\n",
    "x_holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. generating submission file  \n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission['target']=(model.predict(x_holdout)>0.5).astype(int)\n",
    "sample_submission.to_csv('submission0407_LSTM_v2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
